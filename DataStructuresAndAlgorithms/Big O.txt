Big O is a way comparing the two set of code, how efficient they run, time complexity (which code is faster) measurement is in number of operations time/space(memory) complexity.
Time complexity is not measured in time, it is measured in number of operations to complete something.
Regarding space complexity we measure how memory code takes while it runs.
In some cases space efficiency is priority in other time complexity can be priority.
Omicron or Big O is used for worst case in Data structures (foe example getting the last element in array).

1) First Big O is O(n)
This is classic for(int i=0; i < n; ++i) loop. This function is O(n) - proportional
With Big O we have a few rules for simplification. One of those is Drop constants.
If we have two for loops executed for(int i=0; i < n; ++i) one after another, this function run n+n times = 2n
In this case we will not write O(2n), we always drop the constant and write O(n).
The goal of O(n) is to determine whether complexity grows linearly or exponentialy, not O(2n) or O(3n).

2) Second Big O is O(n^2)
The exaple of O(n^2) is one for loop into another
for(int i = 0; i < n; ++n)
{
	for(int j = 0; j < n; ++j)
	{
		std::cout << i << ' ' << j < std::endl;
	}
}
If we pass number 10 it will print 100 lines.
With O(n^2) complexity grows much faster than O(n) -> (O(n^2) is lot less efficient that O(n)).
O(n^2) is the least efficient Big O!

Another way to simplify Big O is Drop Non-Dominants
If we have nested for loop, and if we have additional single for loop, after nested for loops, complexity will be O(n^2 + n), n^2 is dominant, and n is non-dominant,
so if n is larger and larger, than n will be less significant. We simplify this by droping non-dominant n and it will be O(n^2).

3) Next Big O is O(1)
If we have function
int addItems(int n)
{
	return n + n;
}
Here number of operations does not change as n becomes larger, here we have one operation which is addition. This is O(1); If we have n+n+n we will have two operations,
and that will be O(2), but we will simplify this and write O(1).
O(1) is the most efficient Big O! If n becomes larger, the number of operations will not change.

4) Next Big O is O(log n)
If we have list from 1 to 8, (we have to have sorted data), and we have to find the mot efficient way to find the number in a list (we don't know where is the number in the list).
We will divide whole list on a half, and we will say is it in a first half or is it in a second half. If number is not in a second half, we will remove it and we will agan divide rest 
of number on a half, again we will check where is not the number and will remove it, and again we will divide remaining list on a half, until we have single number which is actually
number that we are looking for. For 8 items we had 3 steps to find a number (2^3 = 8). This is O(log n). For example if we have 4,294,967,296 numbers, we need 32 steps to find a number.
O(log n) is wery flat, not as O(1), but is far more efficient than O(n).

5) There is O(nlog n) used in some sorting algorithms -> Merge Sort and Quick sort (the most efficient we can make with sorting algorithm with non numbers only but various sort of data).

O(1) - the best, O(log n), O(n), O(nlog n), O(n^2) - the worst

Different terms for inputs

We had  function int addItems(int n), now, if we have for example int addItems(int a, int b) where we will go through two for loop one after another, we will not have O(n) + O(n) = O(2n) = O(n), but  will have O(a+b)
That means in this case if we have int addItems(int a, int b) but we have two loops one nested into another, complexity will be O(a*b).

