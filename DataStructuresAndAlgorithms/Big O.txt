Big O is a way comparing the two set of code, how efficient they run, time complexity (which code is faster) measurement is in number of operations time/space(memory) complexity.
Time complexity is not measured in time, it is measured in number of operations to complete something.
Regarding space complexity we measure how memory code takes while it runs.
In some cases space efficiency is priority in other time complexity can be priority.
Omicron or Big O is used for worst case in Data structures (foe example getting the last element in array).

1) First Big O is O(n)
This is classic for(int i=0; i < n; ++i) loop. This function is O(n) - proportional
With Big O we have a few rules for simplification. One of those is Drop constants.
If we have two for loops executed for(int i=0; i < n; ++i) one after another, this function run n+n times = 2n
In this case we will not write O(2n), we always drop the constant and write O(n).
The goal of O(n) is to determine whether complexity grows linearly or exponentialy, not O(2n) or O(3n).

2) Second Big O is O(n^2)
The exaple of O(n^2) is one for loop into another
for(int i = 0; i < n; ++n)
{
	for(int j = 0; j < n; ++j)
	{
		std::cout << i << ' ' << j < std::endl;
	}
}
If we pass number 10 it will print 100 lines.
With O(n^2) complexity grows much faster than O(n) -> (O(n^2) is lot less efficient that O(n)).
O(n^2) is the least efficient Big O!

Another way to simplify Big O is Drop Non-Dominants
If we have nested for loop, and if we have additional single for loop, after nested for loops, complexity will be O(n^2 + n), n^2 is dominant, and n is non-dominant,
so if n is larger and larger, than n will be less significant. We simplify this by droping non-dominant n and it will be O(n^2).

3) Next Big O is O(1)
If we have function
int addItems(int n)
{
	return n + n;
}
Here number of operations does not change as n becomes larger, here we have one operation which is addition. This is O(1); If we have n+n+n we will have two operations,
and that will be O(2), but we will simplify this and write O(1).
O(1) is the most efficient Big O! If n becomes larger, the number of operations will not change.

4) Next Big O is O(log n)
If we have list from 1 to 8, (we have to have sorted data), and we have to find the mot efficient way to find the number in a list (we don't know where is the number in the list).
We will divide whole list on a half, and we will say is it in a first half or is it in a second half. If number is not in a second half, we will remove it and we will agan divide rest 
of number on a half, again we will check where is not the number and will remove it, and again we will divide remaining list on a half, until we have single number which is actually
number that we are looking for. For 8 items we had 3 steps to find a number (2^3 = 8). This is O(log n). For example if we have 4,294,967,296 numbers, we need 32 steps to find a number.
O(log n) is wery flat, not as O(1), but is far more efficient than O(n).

5) There is O(nlog n) used in some sorting algorithms -> Merge Sort and Quick sort (the most efficient we can make with sorting algorithm with non numbers only but various sort of data).

O(1) - the best, O(log n), O(n), O(nlog n), O(n^2) - the worst

Different terms for inputs

We had  function int addItems(int n), now, if we have for example int addItems(int a, int b) where we will go through two for loop one after another, we will not have O(n) + O(n) = O(2n) = O(n), but  will have O(a+b)
That means in this case if we have int addItems(int a, int b) but we have two loops one nested into another, complexity will be O(a*b).

Big O - Vectors

From Big O perspective for vectors and arrays adding something to the end we don't have to touch any of existing elements. When we remove item from the end (at vectors and arrays) we also don't need to touch any other element.
Because of that ADDING or REMOVING from vector and array will be O(1). Removing element from the begining of the vector we don't have balid indexes so we have to change indexes (1->0), (2->1) etc.
In that way we have to touch every remaining item in vector, or if we want to add that item in the beggining of the vector, we need to touch every element in the vector (index 2->3),(index 1->2) etc.
ADDING or REMOVING from thr beggining of the vector (because we have to touch every element) complexity will be O(n).
Adding or removing element in the middle of the vector complexity should be O(1/2n). Since Big ) is worst case (not average case) and we dropt constants and adding and removing in the middle of the vector will be O(n).

Finding the item BY VALUE at vector we have to iterate through the vector (from beggining to the end) and complexity will be O(n);
Finding the item BY INDEX at vectir we are going directly at that place in one step and that complexity will be O(1).

If n grows, the differences between O(1), O(log n), O(n) and O(n^2) will grow much faster.
We have following terminologies:
O(n^2) - Loop withing a loop
O(n) - Proportional
O(log n) - Divide and Conquer
O(1) - Constant

O(n^2) it worst
O(n), O(log n) and O(1) is something that we should use.
